{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "Módulo3-Notas\nCurso: Data Analysis with Python (IBM)\nMateria: Ciencia y analítica de datos\nMaximiliano Morones Gómez\nMatrícula: A01793815",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Continuous Numerical Variables\"\nContinuous numerical variables are variables that may contain any value within some range. They can be of \ntype \"int64\" or \"float64\". A great way to visualize these variables is by using scatterplots with fitted lines.\n\nTo start understanding the (linear) relationship between an individual variable and the price, we \ncan use \"regplot\" which plots the scatterplot plus the fitted regression line for the data.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Categorical Variables\"\nThese are variables that describe a 'characteristic' of a data unit, and are selected from a small \ngroup of categories. The categorical variables can have the type \"object\" or \"int64\". A good way \nto visualize categorical variables is by using boxplots.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Descriptive Statistical Analysis\"\nThe describe function automatically computes basic statistics for all continuous variables. Any NaN values \nare automatically skipped in these statistics.\n\nThis will show:\nthe count of that variable\nthe mean\nthe standard deviation (std)\nthe minimum value\nthe IQR (Interquartile Range: 25%, 50% and 75%)\nthe maximum value",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Value Counts\"\nValue counts is a good way of understanding how many units of each characteristic/variable we have.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Basics of Grouping\"\nThe \"groupby\" method groups data by different categories. The data is grouped based on one or \nseveral variables, and analysis is performed on the individual groups.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Correlation and Causation\"\nCorrelation: a measure of the extent of interdependence between variables.\n\nCausation: the relationship between cause and effect between two variables.\n\nIt is important to know the difference between these two. Correlation does not imply causation. Determining \ncorrelation is much simpler the determining causation as causation may require independent experimentation.\n\n\"Pearson Correlation\"\nThe Pearson Correlation measures the linear dependence between two variables X and Y.\n\nThe resulting coefficient is a value between -1 and 1 inclusive, where:\n\n1: Perfect positive linear correlation.\n0: No linear correlation, the two variables most likely do not affect each other.\n-1: Perfect negative linear correlation.\nPearson Correlation is the default method of the function \"corr\". Like before, we can calculate the \nPearson Correlation of the of the 'int64' or 'float64' variables.\n\n\"P-value\"\nWhat is this P-value? The P-value is the probability value that the correlation between these two \nvariables is statistically significant. Normally, we choose a significance level of 0.05, which means \nthat we are 95% confident that the correlation between the variables is significant.\n\nBy convention, when the\n\np-value is  \n<\n  0.001: we say there is strong evidence that the correlation is significant.\nthe p-value is  \n<\n  0.05: there is moderate evidence that the correlation is significant.\nthe p-value is  \n<\n  0.1: there is weak evidence that the correlation is significant.\nthe p-value is  \n>\n  0.1: there is no evidence that the correlation is significant.\nWe can obtain this information using \"stats\" module in the \"scipy\" library.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"ANOVA: Analysis of Variance\"\nThe Analysis of Variance (ANOVA) is a statistical method used to test whether there are significant \ndifferences between the means of two or more groups. ANOVA returns two parameters:\n\nF-test score: ANOVA assumes the means of all groups are the same, calculates how much \nthe actual means deviate from the assumption, and reports it as the F-test score. A larger score means \nthere is a larger difference between the means.\n\nP-value: P-value tells how statistically significant our calculated score value is.\n\nIf our price variable is strongly correlated with the variable we are analyzing, we expect ANOVA to \nreturn a sizeable F-test score and a small p-value.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}