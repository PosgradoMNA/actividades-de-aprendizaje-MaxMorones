{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "Módulo4-Notas\nCurso: Data Analysis with Python (IBM)\nMateria: Ciencia y analítica de datos\nMaximiliano Morones Gómez\nMatrícula: A01793815",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Linear Regression and Multiple Linear Regression\"\nLinear Regression\nOne example of a Data Model that we will be using is:\n\nSimple Linear Regression\nSimple Linear Regression is a method to help us understand the relationship between two variables:\n\nThe predictor/independent variable (X)\nThe response/dependent variable (that we want to predict)(Y)\n\nThe result of Linear Regression is a linear function that predicts the response (dependent) variable as a \nfunction of the predictor (independent) variable.\n\nY: Response Variable\nX: Predictor Variables\n\nLinear Function\nYhat= a+bX\n\na refers to the intercept of the regression line, in other words: the value of Y when X is 0\nb refers to the slope of the regression line, in other words: the value with which Y changes when X increases by 1 unit\n\nMultiple Linear Regression\nWhat if we want to predict car price using more than one variable?\n\nIf we want to use more variables in our model to predict car price, we can use Multiple Linear Regression. Multiple\nLinear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship \nbetween one continuous response (dependent) variable and two or more predictor (independent) variables. Most of \nthe real-world regression models involve multiple predictors. We will illustrate the structure by using four \npredictor variables, but these results can generalize to any integer:\n    \nY: Responde Variable\nX_1: Predictor Variable 1\nX_2: Predictor Variable 2\nX_3: Predictor Variable 3\nX_4: Predictor Variable 4\n\na: intercept\nb_1: coefficients of Variable 1\nb_2: coefficients of Variable 2\nb_3: coefficients of Variable 3\nb_4: coefficients of Variable 4\n\nThe equation is given by:\n    Yhat=a+b_1X_1+b_2X_2+b_3X_3+b_4X_4",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Model Evaluation Using Visualization\"\nRegression Plot\nWhen it comes to simple linear regression, an excellent way to visualize the fit of our model is by using regression plots.\n\nThis plot will show a combination of a scattered data points (a scatterplot), as well as the fitted linear \nregression line going through the data. This will give us a reasonable estimate of the relationship between \nthe two variables, the strength of the correlation, as well as the direction (positive or negative correlation).\n\nResidual Plot\nA good way to visualize the variance of the data is to use a residual plot.\n\nWhat is a residual?\n\nThe difference between the observed value (y) and the predicted value (Yhat) is called the residual (e). When\nwe look at a regression plot, the residual is the distance from the data point to the fitted regression line.\n\nSo what is a residual plot?\n\nA residual plot is a graph that shows the residuals on the vertical y-axis and the independent variable on \nthe horizontal x-axis.\n\nWhat do we pay attention to when looking at a residual plot?\n\nWe look at the spread of the residuals:\n\n- If the points in a residual plot are randomly spread out around the x-axis, then a linear model is \nappropriate for the data.\n\nWhy is that? Randomly spread out residuals means that the variance is constant, and thus the linear model\nis a good fit for this data.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Polynomial Regression and Pipelines\"\nPolynomial regression is a particular case of the general linear regression model or multiple linear regression models.\n\nWe get non-linear relationships by squaring or setting higher-order terms of the predictor variables.    ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\"Measures for In-Sample Evaluation\"\nWhen evaluating our models, not only do we want to visualize the results, but we also want a quantitative \nmeasure to determine how accurate the model is.\n\nTwo very important measures that are often used in Statistics to determine the accuracy of a model are:\n\nR^2 / R-squared\nMean Squared Error (MSE)\nR-squared\n\nR squared, also known as the coefficient of determination, is a measure to indicate how close the data \nis to the fitted regression line.\n\nThe value of the R-squared is the percentage of variation of the response variable (y) that is explained by \na linear model.\n\nMean Squared Error (MSE)\n\nThe Mean Squared Error measures the average of the squares of errors. That is, the difference between actual \nvalue (y) and the estimated value (ŷ).",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}